import numpy as np
import pandas as pd
import os
import random
import cv2
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Lambda, Conv2D, Dropout, MaxPooling2D, Flatten, Dense
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.metrics import precision_recall_curve, classification_report

# Define Paths
normal_img_path = r"C:\Users\dinar\PycharmProjects\PeripheralArteriesDisease\dataset\Healthy"
pad_img_path = r"C:\Users\dinar\PycharmProjects\PeripheralArteriesDisease\dataset\disease"

# Get Image Lists
normal_img_files = list(os.listdir(normal_img_path))
pad_img_files = list(os.listdir(pad_img_path))

# Ensure we have enough images
if len(normal_img_files) < 4 or len(pad_img_files) < 4:
    raise ValueError("Error: Not enough images in one or both folders.")

# Split Train/Test Samples
normal_img_train = random.sample(normal_img_files, 4)
pad_img_train = random.sample(pad_img_files, 4)

rest_normal = list(set(normal_img_files) - set(normal_img_train))
rest_pad = list(set(pad_img_files) - set(pad_img_train))

normal_img_test = random.sample(rest_normal, min(4, len(rest_normal)))  # Ensure there's enough test data
pad_img_test = random.sample(rest_pad, min(4, len(rest_pad)))

# Print Selected Samples
print("Training Normal Images:", normal_img_train)
print("Training PAD Images:", pad_img_train)
print("Testing Normal Images:", normal_img_test)
print("Testing PAD Images:", pad_img_test)

# Load Images
def load_images(img_path, img_list):
    img_data = []
    for img_name in img_list:
        image = cv2.imread(os.path.join(img_path, img_name), 1)
        image = cv2.resize(image, (256, 256))
        img_data.append(np.array(image))
    return img_data

train_images = load_images(normal_img_path, normal_img_train) + load_images(pad_img_path, pad_img_train)
test_images = load_images(normal_img_path, normal_img_test) + load_images(pad_img_path, pad_img_test)

train_labels = np.concatenate((np.zeros(len(normal_img_train)), np.ones(len(pad_img_train))), axis=None)
test_labels = np.concatenate((np.zeros(len(normal_img_test)), np.ones(len(pad_img_test))), axis=None)

# Data Augmentation for Training and Generating More Test Samples
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

datagen.fit(np.array(train_images))  # Augment training data
datagen.fit(np.array(test_images))   # Augment test data

train_generator = datagen.flow(np.array(train_images), train_labels, batch_size=20)
test_generator = datagen.flow(np.array(test_images), test_labels, batch_size=10, shuffle=False)

# Split into Training & Validation Sets
X_train, X_val, y_train, y_val = train_test_split(np.array(train_images), train_labels, test_size=0.4, random_state=42)

# Define CNN Model
cnn_model = Sequential([
    InputLayer(input_shape=(256,256,3)),
    Lambda(lambda x: x/255.),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    Dropout(0.1),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Dropout(0.1),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2)),

    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Dropout(0.2),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Dropout(0.2),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(1024, activation="relu"),
    Dense(512, activation="relu"),
    Dense(1, activation="sigmoid")
])

cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the Model Using Augmented Data
history = cnn_model.fit(
    train_generator,
    validation_data=(X_val, y_val),
    epochs=15,
    verbose=1
)

# Evaluate on Test Data
test_loss, test_acc = cnn_model.evaluate(test_generator, verbose=1)
print("Test Accuracy:", test_acc)


y_true = test_labels # Use test lables from earlier
y_pred_probs = cnn_model.predict(test_generator)  # Model predictions (probabilities)

precision, recall, thresholds = precision_recall_curve(y_true, y_pred_probs)
f1_scores = 2 * (precision * recall) / (precision + recall)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

y_pred = (y_pred_probs >= optimal_threshold).astype(int)  # Convert to binary predictions

print(f"Optimal Threshold: {optimal_threshold:.8f}\n")
print(classification_report(y_true, y_pred, target_names=['Class Normal', 'Class PAD']))


# âœ… Plot Model Accuracy & Loss
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))

# Accuracy Graph
ax[0].plot(history.history['accuracy'], label='Train Accuracy')
ax[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
ax[0].set_title('Model Accuracy')
ax[0].set_xlabel('Epoch')
ax[0].set_ylabel('Accuracy')
ax[0].legend()

# Loss Graph
ax[1].plot(history.history['loss'], label='Train Loss')
ax[1].plot(history.history['val_loss'], label='Validation Loss')
ax[1].set_title('Model Loss')
ax[1].set_xlabel('Epoch')
ax[1].set_ylabel('Loss')
ax[1].legend()

plt.show()
